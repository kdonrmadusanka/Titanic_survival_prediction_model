# ğŸš¢ Titanic Survival Prediction â€“ Machine Learning Project

## ğŸ“Œ Project Overview

This project focuses on predicting passenger survival on the Titanic using **machine learning**. It demonstrates a **complete endâ€‘toâ€‘end ML workflow**, including data cleaning, feature engineering, preprocessing with pipelines, model building, and evaluation.

The project follows **industry best practices** using **pandas** and **scikitâ€‘learn**, ensuring clean, reproducible, and leakageâ€‘free model training.

---

## ğŸ“Š Dataset Description

The dataset contains passenger information with the following key columns:

* **Target Variable**:

  * `Survived` â†’ 0 (Did not survive), 1 (Survived)

* **Numerical Features**:

  * `Age`, `Fare`, `SibSp`, `Parch`

* **Categorical Features**:

  * `Sex`, `Embarked`, `Cabin`

---

## ğŸ§¹ Data Cleaning & Preprocessing

### 1ï¸âƒ£ Target Separation

To avoid data leakage, the target variable was separated before preprocessing:

* **X (features)** â†’ All columns except `Survived`
* **y (target)** â†’ `Survived`

---

### 2ï¸âƒ£ Handling Missing Values

| Column   | Strategy          | Reason                         |
| -------- | ----------------- | ------------------------------ |
| Age      | Mean Imputation   | Continuous numeric feature     |
| Fare     | Median Imputation | Robust to outliers             |
| Embarked | Most Frequent     | Small categorical domain       |
| Cabin    | Dropped / Binary  | High cardinality, lost meaning |

---

### 3ï¸âƒ£ Feature Engineering

#### ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ Family Size

A new feature was created:

```
FamilySize = SibSp + Parch + 1
```

The original columns `SibSp` and `Parch` were dropped after feature creation.

---

### 4ï¸âƒ£ Encoding Categorical Variables

* **Sex** â†’ Binary encoding (`male = 0`, `female = 1`)
* **Embarked** â†’ Oneâ€‘Hot Encoding with `drop_first=True`

This avoids artificial ordering and multicollinearity.

---

### 5ï¸âƒ£ Feature Scaling

| Feature | Transformation         |
| ------- | ---------------------- |
| Age     | StandardScaler         |
| Fare    | log1p â†’ StandardScaler |

**Reasoning:**

* `Fare` is highly rightâ€‘skewed
* Log transformation makes it closer to a bellâ€‘shaped distribution
* Scaling improves performance for linear models

---

### 6ï¸âƒ£ Pipeline & ColumnTransformer

All preprocessing steps were implemented using **Pipeline** and **ColumnTransformer**, ensuring:

* Correct columnâ€‘wise transformations
* No data leakage
* Automatic reuse for validation and test sets

---

## ğŸ¤– Model Building

### Algorithm Used

* **Logistic Regression**

**Why Logistic Regression?**

* Wellâ€‘suited for binary classification
* Interpretable
* Performs well with scaled features

---

### Model Pipeline

The full pipeline consists of:

1. Preprocessing (imputation, scaling, encoding)
2. Logistic Regression classifier

This ensures that raw data can be passed directly into the model.

---

## ğŸ§ª Model Evaluation

### 1ï¸âƒ£ Validation Accuracy

Model performance was evaluated using a validation set.

* **Accuracy Score**: Measures overall correctness of predictions

---

### 2ï¸âƒ£ Confusion Matrix

The confusion matrix was used to analyze prediction errors:

|            | Predicted No    | Predicted Yes   |
| ---------- | --------------- | --------------- |
| Actual No  | True Negatives  | False Positives |
| Actual Yes | False Negatives | True Positives  |

This helps understand:

* Survival prediction errors
* Model bias toward any class

---

### 3ï¸âƒ£ Classification Report

The following metrics were analyzed:

* **Precision** â†’ Correct positive predictions
* **Recall** â†’ Ability to detect survivors
* **F1â€‘Score** â†’ Balance between precision and recall

---

## ğŸ“ˆ Results Summary

* The model successfully learned survival patterns from passenger data
* Feature engineering (FamilySize, Fare log transform) improved performance
* Pipelineâ€‘based preprocessing ensured consistent results on unseen data

---

## ğŸ§  Key Learnings

* Proper data cleaning is crucial for model performance
* Log transformation helps handle skewed numerical features
* Pipelines prevent data leakage and simplify ML workflows
* Confusion matrices provide deeper insight than accuracy alone

---

## ğŸš€ Future Improvements

* Try advanced models (Random Forest, XGBoost)
* Hyperparameter tuning
* Crossâ€‘validation
* Feature importance analysis

---

## ğŸ›  Technologies Used

* Python
* pandas
* NumPy
* scikitâ€‘learn
* Matplotlib

---

## ğŸ“Œ Conclusion

This project demonstrates a **complete and professional machine learning workflow**, from raw data cleaning to model evaluation. The use of pipelines and transformers ensures scalability, reproducibility, and correctness, making this project suitable for academic submission and realâ€‘world ML applications.

---

â­ *Feel free to fork this repository and experiment with different models!*
